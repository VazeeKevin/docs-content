---
meta:
title: How to upgrade your Kubernetes Kapsule cluster using the CLI
description: This page explains how to upgrade your Kubernetes Kapsule cluster using the CLI
content:
h1: How to upgrade your Kubernetes Kapsule cluster using the CLI
paragraph: This page explains how to upgrade your Kubernetes Kapsule cluster using the CLI
tags: kubernetes kubernetes-kapsule kapsule cluster cli kubectl
dates:
categories:
  - kubernetes
---

<Macro id="iam-requirements" />

<Message type="requirement">
    - You have a working [CLI](https://github.com/scaleway/scaleway-cli) with your credentials set up
    - You are willing to upgrade your Kubernetes Kapsule cluster to the latest k8s version available on the Kapsule API.
    you're willing to upgrade it.
</Message>

First, we need to ensure that the latest Kapsule version properly handles your workload. We have a compatibility matrix for different components and your current cluster may use components that are either deprecated or not available on the latest version.
For more information, you can refer to our [version policy](https://www.scaleway.com/en/docs/containers/kubernetes/reference-content/version-support-policy/).

We recommend you read the Kubernetes [changelog](https://kubernetes.io/releases/) to stay informed on the latest version upgrade.

## Checking which components must be changed

Run the following command into your terminal to retrieve a list of the components that need to be changed.  

`scw k8s version list`

You should get an output similar to the following, providing a list of all relevant components:
```
1.25.9   [cilium calico kilo]                [none]                 [containerd]
1.24.13  [cilium calico weave flannel kilo]  [none]                 [containerd crio]
1.23.17  [cilium calico weave flannel kilo]  [none nginx traefik2]  [containerd crio docker]
```

### Ingresses

Managed Ingress Controllers have been deprecated since the minor 1.24 release. As a result, we provide the EasyDeploy feature instead to set up an Ingress Controller. 
You can also deploy an ingress controller by yourself to fine tune its settings.

To update a cluster to a version superior to 1.23, this command is necessary to deactivate the managed ingress controller
`scw k8s cluster update $CLUSTER_ID ingress=none`

### Container runtimes

We only provide support for containerd from 1.25 and above. You will need to create new Kapsule pools with containerd as a runtime to migrate your existing pools.
Here's how:
1. Create the pool
  `scw k8s pool create container-runtime=containerd zone=$POOL_ZONE size=$SIZE_OF_YOUR_OLD_POOL version=$YOUR_CLUSTER_VERSION cluster-id=$CLUSTER_ID`
2. Wait for the nodes to be provisioned
  `scw k8s pool wait $POOL_ID`
3. In parallel, you can start cordoning nodes using the old runtime. This way, the workload will get rescheduled directly on the `containerd` nodes.
  `kubectl cordon $OLD_NODE_A $OLD_NODE_B $OLD_NODE_C ...`
4. Once the new nodes are ready, you can start draining the old nodes, you may need to add the option `--delete-emptydir-data` if you used local disk as a scratchpad.
  `kubectl drain --ignore-daemonsets $NODE_TO_DRAIN`
    <Message type="important">
        **Do not** drain all the nodes simultaneously. Make sure to do it sequentially, while checking your workload is behaving as expected.
    </Message>
5. At this point, nodes from your old pool should be empty of any workload. If so, they are ready for deletion.
  `kubectl delete node $OLD_NODE_A $OLD_NODE_B $OLD_NODE_C ...`
6. Finally, delete your old pool
  `scw k8s pool delete $OLD_POOL_ID`

### CNIs

If your cluster is using a deprecated CNI, and you are willing to upgrade to a newer k8s version, you will most likely have to spin up a new Kapsule cluster.
As of now, we do not provide an easy way to change CNI. This component is integrated with each cluster node which results in a tricky transition.
Do not hesitate to contact us on our [Slack community](scaleway-community.slack.com) channel #k8s or open a support ticket if you have any issues.

## Effective Upgrade

From here, two options are available: you are either upgrading **one minor version** or **multiple ones**.

### One minor version

This is pretty straightforward, you will first upgrade your control-plane.
`scw k8s cluster upgrade $CLUSTER_ID version=$NEW_K8S_VERSION`
You can choose to upgrade the pools as well, if so append the previous command with `upgrade-pools=true`

You can also upgrade one pool independently with
`scw k8s pool upgrade $POOL_ID version=$NEW_K8S_VERSION`

You can be extra cautious and migrate manually your workload. To do so, follow the steps described in [runtimes](#container-runtimes) section,
just adapt the pool creation step
`scw k8s pool create zone=$OLD_POOL_ZONE size=$SIZE_OF_YOUR_OLD_POOL version=$NEW_CLUSTER_VERSION cluster-id=$CLUSTER_ID`

Monitor your workload, congrats you are using a newer, maybe the latest kubernetes version.

### Multiple minors versions

The process is quite similar to the previous one except you need to repeat the steps for each minor version.
<Message type="important">
    Your pools and your control plane **should never** have more than one minor version delta.
</Message>

<Navigation title="See Also">
  <PreviousButton to="/containers/kubernetes/how-to/enable-disable-ssh/">How to enable/disable SSH</PreviousButton>
</Navigation>